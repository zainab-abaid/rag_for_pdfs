"""
Ingest PDF section-wise nodes into PostgreSQL with pgvector embeddings via LlamaIndex.

Workflow:

    PDF Files
       │
       ▼
    PDF Parsing → Nodes (.pkl files with text + metadata)
       │
       ▼
    Ingest Script (this file)
       │
       ├─ Load nodes
       ├─ Generate embeddings via OpenAI async API
       ├─ Batch & retry logic for speed + reliability
       └─ Insert text + embeddings + metadata into Postgres
       │
       ▼
    PostgreSQL + pgvector table (vector index for semantic search)
       │
       ▼
    Retrieval / RAG / Semantic Search

Features:

- Reads `.pkl` node files generated by PDF parsing scripts (each node contains a text chunk + metadata).
- Generates embeddings for the text content using OpenAI embeddings API (`text-embedding-3-small` by default).
- Persists both text and metadata into a PostgreSQL table with a vector column for semantic search.
- Creates a pgvector IVFFLAT index for fast similarity search.
- Handles batching, concurrency, and retries for efficient and reliable embedding generation.

Environment variables (from .env):

  PG_HOST=localhost                # Postgres host
  PG_PORT=5432                     # Postgres port
  PG_DB=extreme_pdfs               # Database name
  PG_SCHEMA=extreme_pdfs_schema    # Schema name
  PG_TABLE_PDF=pdf_chunks          # Table to store chunks + embeddings
  PG_USER=xxx                      # Database user
  PG_PASSWORD=xxx                  # Database password
  OVERWRITE_TABLE=true             # Drop table before ingestion if true

  EMBED_MODEL=text-embedding-3-small    # OpenAI embedding model
  EMBED_DIM=1536                        # Dimension of embeddings
  EMBED_BATCH_SIZE=100                  # Number of nodes per API request

  PDF_PARSED_OUTPUT_DIR=./data/pdf_node_chunks   # Directory containing node .pkl files

Design choices:

- Uses LlamaIndex nodes instead of JSON to preserve metadata and text structure natively.
- Async + concurrency for fast embedding generation without exceeding API limits.
- Batching for efficiency and memory safety.
- Executemany inserts and vector indexing for high-performance retrieval.
- Retry logic ensures resilience against transient API or network errors.
- Progress bars provide operational visibility during ingestion.
"""

import os
import pickle
from pathlib import Path
from typing import List, Dict, Any

import psycopg
from dotenv import load_dotenv
from openai import OpenAI
import json

load_dotenv()

# ---------------- Env/Configs ----------------
PG_HOST = os.getenv("PG_HOST", "localhost")
PG_PORT = int(os.getenv("PG_PORT", 5432))
PG_DB = os.getenv("PG_DB")
PG_SCHEMA = os.getenv("PG_SCHEMA", "extreme_pdfs_schema")
PG_TABLE_PDF = os.getenv("PG_TABLE_PDF", "pdf_chunks")
PG_USER = os.getenv("PG_USER")
PG_PASSWORD = os.getenv("PG_PASSWORD", "")
OVERWRITE_TABLE = os.getenv("OVERWRITE_TABLE", "false").lower() == "true"

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
EMBED_DIM = int(os.getenv("EMBED_DIM", "1536"))
EMBED_BATCH_SIZE = int(os.getenv("EMBED_BATCH_SIZE", "100"))

NODES_DIR = Path(os.getenv("PDF_PARSED_OUTPUT_DIR", "./data/pdf_node_chunks"))


#------------------Utility Functions-----------
def load_nodes(node_file: Path) -> List[Dict[str, Any]]:
    """Load nodes from a .pkl file"""
    with open(node_file, "rb") as f:
        return pickle.load(f)
    
def create_table_if_needed(conn):
    """Create table for storing PDF chunks and embeddings"""
    fqtn = f'"{PG_SCHEMA}"."{PG_TABLE_PDF}"'
    with conn.cursor() as cur:
        if OVERWRITE_TABLE:
            print(f"[ingest] Dropping table {fqtn} (OVERWRITE_TABLE=True)...")
            cur.execute(f"DROP TABLE IF EXISTS {fqtn} CASCADE;")

        # Create table if not exists
        cur.execute(f"""
                CREATE TABLE IF NOT EXISTS {fqtn} (
                id SERIAL PRIMARY KEY,
                source_file TEXT NOT NULL,
                chunk_index INT NOT NULL,
                text TEXT NOT NULL,
                embedding vector({EMBED_DIM}) NOT NULL,
                metadata jsonb NOT NULL,
                text_search_tsv tsvector
            );
        """)
        conn.commit()
    
    print(f"[ingest] Table ready: {fqtn}")

def generate_embedding(client: OpenAI, text: str):
    """Generate embedding vector using OpenAI"""
    response = client.embeddings.create(
        model=EMBED_MODEL,
        input=text
    )
    return response.data[0].embedding

def insert_nodes(conn, nodes: List[Dict[str, Any]], client: OpenAI, batch_size: int = 100):
    """Insert nodes into PostgreSQL with embeddings, batching, metadata, and tsvector"""
    fqtn = f'"{PG_SCHEMA}"."{PG_TABLE_PDF}"'
    total_inserted = 0
    batch_nodes = []
    batch_texts = []

    def flush_batch():
        nonlocal batch_nodes, batch_texts, total_inserted
        if not batch_nodes:
            return

        # Generate embeddings for batch
        response = client.embeddings.create(model=EMBED_MODEL, input=batch_texts)
        embeddings = [item.embedding for item in response.data]

        # Prepare rows for insertion
        rows_to_insert = [
            (
                txt,  # text
                json.dumps({"doc_title": os.path.basename(src)}),  # metadata
                idx,  # chunk_index
                src,  # source_file
                emb,  # embedding
                txt   # for tsvector
            )
            for (src, idx, txt), emb in zip(batch_nodes, embeddings)
        ]

        # Insert batch
        with conn.cursor() as cur:
            cur.executemany(
                f"""INSERT INTO {fqtn} (text, metadata, chunk_index, source_file, embedding, text_search_tsv) VALUES (%s, %s, %s, %s, %s, to_tsvector('english', %s))""",
                rows_to_insert
            )
        conn.commit()

        total_inserted += len(rows_to_insert)
        if total_inserted % 500 == 0:
            print(f"[ingest] {total_inserted} nodes inserted so far...")

        batch_nodes.clear()
        batch_texts.clear()

    # ---------------- Process nodes ----------------
    for node in nodes:
        text = node.text
        source_file = node.metadata.get("source_file", "unknown.pdf")
        chunk_index = node.metadata.get("chunk_index", 0)

        batch_nodes.append((source_file, chunk_index, text))
        batch_texts.append(text)

        if len(batch_nodes) >= batch_size:
            flush_batch()

    # Flush any remaining nodes
    flush_batch()
    print(f"[ingest] Inserted {total_inserted} nodes into {fqtn}")


def main():
    # Connect to PostgreSQL
    with psycopg.connect(
        host=PG_HOST,
        port=PG_PORT,
        dbname=PG_DB,
        user=PG_USER,
        password=PG_PASSWORD
    ) as conn:
        create_table_if_needed(conn)

        # Initialize OpenAI client
        client = OpenAI()

        # Discover node files
        node_files = sorted(NODES_DIR.rglob("*.pkl"))
        print(f"[ingest] Found {len(node_files)} node files in {NODES_DIR}")
        print(f"[ingest] Embedding model: {EMBED_MODEL}")
        print(f"[ingest] Embed batch size: {EMBED_BATCH_SIZE}")
        print("-" * 60)

        grand_total = 0

        for idx, node_file in enumerate(node_files, start=1):
            nodes = load_nodes(node_file)

            print(f"[ingest] ({idx}/{len(node_files)}) Processing {node_file.relative_to(NODES_DIR)} ({len(nodes)} nodes)")

            insert_nodes(conn, nodes, client, batch_size=EMBED_BATCH_SIZE)
            grand_total += len(nodes)

        print("-" * 60)
        print(f"[ingest] DONE — total nodes processed: {grand_total}")

if __name__ == "__main__":
    main()