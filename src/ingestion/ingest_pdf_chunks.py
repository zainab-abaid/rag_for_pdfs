"""
Ingest PDF section-wise nodes into PostgreSQL with pgvector embeddings via LlamaIndex.

Workflow:

    PDF Files
       │
       ▼
    PDF Parsing → Nodes (.pkl files with text + metadata)
       │
       ▼
    Ingest Script (this file)
       │
       ├─ Load nodes
       ├─ Generate embeddings via OpenAI async API
       ├─ Batch & retry logic for speed + reliability
       └─ Insert text + embeddings + metadata into Postgres
       │
       ▼
    PostgreSQL + pgvector table (vector index for semantic search)
       │
       ▼
    Retrieval / RAG / Semantic Search

Features:

- Reads `.pkl` node files generated by PDF parsing scripts (each node contains a text chunk + metadata).
- Generates embeddings for the text content using OpenAI embeddings API (`text-embedding-3-small` by default).
- Persists both text and metadata into a PostgreSQL table with a vector column for semantic search.
- Creates a pgvector IVFFLAT index for fast similarity search.
- Handles batching, concurrency, and retries for efficient and reliable embedding generation.

Environment variables (from .env):

  PG_HOST=localhost                # Postgres host
  PG_PORT=5432                     # Postgres port
  PG_DB=extreme_pdfs               # Database name
  PG_SCHEMA=extreme_pdfs_schema    # Schema name
  PG_TABLE_PDF=pdf_chunks          # Table to store chunks + embeddings
  PG_USER=xxx                      # Database user
  PG_PASSWORD=xxx                  # Database password
  OVERWRITE_TABLE=true             # Drop table before ingestion if true

  EMBED_MODEL=text-embedding-3-small    # OpenAI embedding model
  EMBED_DIM=1536                        # Dimension of embeddings
  EMBED_BATCH_SIZE=100                  # Number of nodes per API request
  MAX_CONCURRENT=5                      # Number of parallel embedding requests

  PDF_PARSED_OUTPUT_DIR=./data/pdf_node_chunks   # Directory containing node .pkl files

Design choices:

- Uses LlamaIndex nodes instead of JSON to preserve metadata and text structure natively.
- Async + concurrency for fast embedding generation without exceeding API limits.
- Batching for efficiency and memory safety.
- Executemany inserts and vector indexing for high-performance retrieval.
- Retry logic ensures resilience against transient API or network errors.
- Progress bars provide operational visibility during ingestion.
"""

import os
import pickle
import asyncio
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

import psycopg
from dotenv import load_dotenv
from openai import AsyncOpenAI

load_dotenv()

# ---------------- Env/Configs ----------------
PG_HOST = os.getenv("PG_HOST", "localhost")
PG_PORT = int(os.getenv("PG_PORT", 5432))
PG_DB = os.getenv("PG_DB")
PG_SCHEMA = os.getenv("PG_SCHEMA", "extreme_pdfs_schema")
PG_TABLE_PDF = os.getenv("PG_TABLE_PDF", "pdf_chunks")
PG_USER = os.getenv("PG_USER")
PG_PASSWORD = os.getenv("PG_PASSWORD", "")
OVERWRITE_TABLE = os.getenv("OVERWRITE_TABLE", "false").lower() == "true"

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
EMBED_DIM = int(os.getenv("EMBED_DIM", "1536"))
EMBED_BATCH_SIZE = int(os.getenv("EMBED_BATCH_SIZE", "100")) 
MAX_CONCURRENT = int(os.getenv("MAX_CONCURRENT", "5"))  # Parallel API calls

NODES_DIR = Path(os.getenv("PDF_PARSED_OUTPUT_DIR", "./data/pdf_node_chunks"))


#------------------Utility Functions-----------
def load_nodes(node_file: Path) -> List[Any]:
    """Load nodes from a .pkl file"""
    with open(node_file, "rb") as f:
        return pickle.load(f)
    

def create_table_if_needed(conn):
    """Create table for storing PDF chunks and embeddings"""
    fqtn = f'"{PG_SCHEMA}"."{PG_TABLE_PDF}"'
    with conn.cursor() as cur:
        if OVERWRITE_TABLE:
            print(f"[ingest] Dropping table {fqtn} (OVERWRITE_TABLE=True)...")
            cur.execute(f"DROP TABLE IF EXISTS {fqtn} CASCADE;")

        # Create table if not exists
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {fqtn} (
                id SERIAL PRIMARY KEY,
                source_file TEXT NOT NULL,
                chunk_index INT NOT NULL,
                text TEXT NOT NULL,
                embedding vector({EMBED_DIM}) NOT NULL
            );
        """)
        
        # Check if index exists
        cur.execute(f"""
            SELECT indexname FROM pg_indexes 
            WHERE schemaname = %s AND tablename = %s AND indexname = 'pdf_chunks_embedding_idx';
        """, (PG_SCHEMA, PG_TABLE_PDF))
        
        if not cur.fetchone():
            print(f"[ingest] Creating vector index (this may take a while)...")
            cur.execute(f"""
                CREATE INDEX pdf_chunks_embedding_idx 
                ON {fqtn} USING ivfflat (embedding vector_cosine_ops)
                WITH (lists = 100);
            """)
        
        conn.commit()
    
    print(f"[ingest] Table ready: {fqtn}")


async def generate_embeddings_batch(client: AsyncOpenAI, texts: List[str], max_retries: int = 3) -> List[List[float]]:
    """Generate embeddings with retry logic"""
    for attempt in range(max_retries):
        try:
            response = await client.embeddings.create(
                model=EMBED_MODEL,
                input=texts
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt  # Exponential backoff
            print(f"[ingest] API error (attempt {attempt + 1}/{max_retries}): {e}")
            print(f"[ingest] Retrying in {wait_time}s...")
            await asyncio.sleep(wait_time)


async def process_batch(client: AsyncOpenAI, batch_data: List[tuple], semaphore: asyncio.Semaphore) -> List[tuple]:
    """Process a single batch with rate limiting"""
    async with semaphore:
        texts = [text for _, _, text in batch_data]
        embeddings = await generate_embeddings_batch(client, texts)
        return [
            (src, idx, txt, emb) 
            for (src, idx, txt), emb in zip(batch_data, embeddings)
        ]


async def insert_nodes_async(conn_params: dict, nodes: List[Any], client: AsyncOpenAI, batch_size: int = 2048):
    """Insert nodes into PostgreSQL with async embedding generation"""
    fqtn = f'"{PG_SCHEMA}"."{PG_TABLE_PDF}"'
    
    # Prepare all data
    all_data = []
    for node in nodes:
        text = node.text
        source_file = node.metadata.get("source_file", "unknown.pdf")
        chunk_index = node.metadata.get("chunk_index", 0)
        all_data.append((source_file, chunk_index, text))
    
    # Split into batches
    batches = [all_data[i:i + batch_size] for i in range(0, len(all_data), batch_size)]
    
    # Semaphore to limit concurrent API calls
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    
    # Process all batches concurrently with progress bar
    print(f"[ingest] Processing {len(batches)} batches ({len(nodes)} nodes)...")
    
    tasks = [process_batch(client, batch, semaphore) for batch in batches]
    results = []
    
    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="Embedding"):
        result = await coro
        results.append(result)
    
    # Insert all results into database
    print(f"[ingest] Inserting {len(nodes)} nodes into database...")
    with psycopg.connect(**conn_params) as conn:
        with conn.cursor() as cur:
            for batch_result in tqdm(results, desc="Inserting"):
                cur.executemany(
                    f"INSERT INTO {fqtn} (source_file, chunk_index, text, embedding) VALUES (%s, %s, %s, %s)",
                    batch_result
                )
        conn.commit()
    
    print(f"[ingest] ✓ Inserted {len(nodes)} nodes")


async def main_async():
    # Connection parameters
    conn_params = {
        "host": PG_HOST,
        "port": PG_PORT,
        "dbname": PG_DB,
        "user": PG_USER,
        "password": PG_PASSWORD
    }
    
    # Setup database
    with psycopg.connect(**conn_params) as conn:
        create_table_if_needed(conn)
    
    # Initialize async OpenAI client with context manager
    async with AsyncOpenAI() as client:
        # Discover node files
        node_files = sorted(NODES_DIR.rglob("*.pkl"))
        print(f"\n[ingest] Found {len(node_files)} node files in {NODES_DIR}")
        print(f"[ingest] Embedding model: {EMBED_MODEL}")
        print(f"[ingest] Batch size: {EMBED_BATCH_SIZE}")
        print(f"[ingest] Max concurrent API calls: {MAX_CONCURRENT}")
        print("-" * 60)
        
        grand_total = 0
        
        for idx, node_file in enumerate(node_files, start=1):
            nodes = load_nodes(node_file)
            
            print(
                f"\n[ingest] ({idx}/{len(node_files)}) "
                f"Processing {node_file.relative_to(NODES_DIR)} "
                f"({len(nodes)} nodes)"
            )
            
            await insert_nodes_async(
                conn_params=conn_params,
                nodes=nodes,
                client=client,
                batch_size=EMBED_BATCH_SIZE
            )
            
            grand_total += len(nodes)
        
        print("\n" + "=" * 60)
        print(f"[ingest] ✓ DONE — total nodes processed: {grand_total}")
        print("=" * 60)


def main():
    asyncio.run(main_async())


if __name__ == "__main__":
    main()